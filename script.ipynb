{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f55633-3457-4831-951a-20c13cd1aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c36e1-cad5-4d04-915d-143e76d87379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and preprocessing\n",
    "# Using MSCOCO Dataset here\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define image and text preprocessing transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ImageTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annFile, transform, tokenizer):\n",
    "        self.dataset = CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, captions = self.dataset[idx]\n",
    "        # Choose the first caption\n",
    "        text = captions[0]\n",
    "        text_encoded = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=32)\n",
    "        return image, text_encoded['input_ids'], text_encoded['attention_mask']\n",
    "\n",
    "# Example usage\n",
    "dataset = ImageTextDataset(root='C:/Users/ADMIN/Downloads/TextBasedImageQuery/dataset/images/val2017', annFile='C:/Users/ADMIN/Downloads/TextBasedImageQuery/dataset/annotations/captions_val2017.json', transform=image_transform, tokenizer=tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    drop_last=True  # Drop incomplete batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c70be8-ae8e-4e3a-a303-05f5304e5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class TextImageModel(torch.nn.Module):\n",
    "    def __init__(self, text_encoder, image_encoder):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_projection = torch.nn.Linear(768, 512)\n",
    "        self.image_projection = torch.nn.Linear(2048, 512)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        text_features = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        text_features = self.text_projection(text_features)\n",
    "        \n",
    "        image_features = self.image_encoder(images).squeeze()\n",
    "        image_features = self.image_projection(image_features)\n",
    "        \n",
    "        return text_features, image_features\n",
    "\n",
    "# Load pretrained encoders\n",
    "\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Load pretrained weights\n",
    "image_encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "image_encoder = torch.nn.Sequential(*(list(image_encoder.children())[:-1]))\n",
    "\n",
    "# Initialize the model\n",
    "model = TextImageModel(text_encoder, image_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d9e63-fe11-4966-b334-c193a05d9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "criterion = torch.nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader):\n",
    "        input_ids, attention_mask, images = input_ids.squeeze(1), attention_mask.squeeze(1), images\n",
    "        text_features, image_features = model(input_ids, attention_mask, images)\n",
    "        \n",
    "        # Target: 1 (similar embeddings)\n",
    "        targets = torch.ones(text_features.size(0)).to(images.device)\n",
    "        loss = criterion(text_features, image_features, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c4314b-042f-4d00-b08e-9ebcba1ee05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Function to compute Recall@K\n",
    "def recall_at_k(model, dataloader, k=5):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in dataloader:\n",
    "            input_ids, attention_mask, images = input_ids.squeeze(1), attention_mask.squeeze(1), images\n",
    "            text_features, image_features = model(input_ids, attention_mask, images)\n",
    "            similarities = torch.matmul(text_features, image_features.T)\n",
    "            _, indices = similarities.topk(k, dim=1)\n",
    "            correct += sum(i in indices[i] for i in range(images.size(0)))\n",
    "            total += images.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Compute metrics\n",
    "r5 = recall_at_k(model, dataloader, k=5)\n",
    "print(f\"Recall@5: {r5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd6d96-f08b-48fb-a3f9-a6062c6d4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'text_image_model.pt')\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('text_image_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7884e4-0f98-4d46-99c4-5edb7fe24c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "def batched_inference(text_query, model, dataloader, device, k=5):\n",
    "    model.eval()\n",
    "    similarities = []\n",
    "    all_images = []  # To store all images in a flattened list\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            image_features = model.image_encoder(images).squeeze()\n",
    "            image_features = model.image_projection(image_features)\n",
    "\n",
    "            # Handle singleton dimensions\n",
    "            if image_features.dim() == 1:\n",
    "                image_features = image_features.unsqueeze(0)\n",
    "\n",
    "            # Compute text features once\n",
    "            text_features = model.text_encoder(**text_query).pooler_output\n",
    "            text_features = model.text_projection(text_features)\n",
    "\n",
    "            # Compute similarity\n",
    "            similarity = torch.matmul(text_features, image_features.T)\n",
    "            similarities.append(similarity.cpu())\n",
    "\n",
    "            # Flatten images into a list\n",
    "            all_images.extend(images.cpu())  # Store images in a flattened list\n",
    "\n",
    "    # Concatenate all similarities\n",
    "    all_similarities = torch.cat(similarities)\n",
    "    top_k_scores, top_k_indices = all_similarities.topk(k)\n",
    "\n",
    "    # Retrieve top-k images and scores\n",
    "    top_k_indices = top_k_indices.view(-1).tolist()\n",
    "    top_k_images = [(all_images[idx.item()], top_k_scores[i].item()) for i, idx in enumerate(top_k_indices)]\n",
    "\n",
    "    return top_k_images\n",
    "\n",
    "\n",
    "# Initialize tokenizer and tokenize the query\n",
    "query_text = \"A dog playing with a ball\"\n",
    "text_query = tokenizer(query_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32)\n",
    "text_query = {k: v.to(device) for k, v in text_query.items()}\n",
    "\n",
    "# Perform batched inference to get top-k results\n",
    "top_k_images = batched_inference(text_query, model, dataloader, device, k=5)\n",
    "\n",
    "# Print similarity scores and visualize images\n",
    "for idx, (image_tensor, score) in enumerate(top_k_images):\n",
    "    print(f\"Rank {idx+1}, Similarity Score: {score:.4f}\")\n",
    "\n",
    "    # Visualize the image\n",
    "    image = to_pil_image(image_tensor)\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e646057-07cd-485f-973d-3aaef25e5487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
